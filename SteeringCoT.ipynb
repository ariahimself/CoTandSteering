{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB7x_eqmuqry",
        "outputId": "cb90bea5-55f0-405b-99a3-97da5f21e5e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install \"transformers>=4.44.0\" \"accelerate>=0.34.0\" \"bitsandbytes>=0.44.1\" \"torch>=2.3.0\" \"einops\" \"sentencepiece\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # change here if you like\n"
      ],
      "metadata": {
        "id": "AfuE6oBjuwPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEEfktxqvOAs",
        "outputId": "36db9b1d-6ced-4931-ef63-06bb47555269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.get_device_capability(0)[0] >= 8 else torch.float16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "SF5tpPq5u1JF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "894383eb3645419aba3dc2f0c45b3365",
            "b5a1efccd6c4491ab129ec8304e0a027",
            "6475223257574c9e8c7b3bc3abae65f7",
            "a4b97af97b6940fea1ccc8dc566e0046",
            "389b04215ea3493785986ae4b15a5a98",
            "8c0d58d160c34aceb61f11364439f444",
            "ec5cc2917f054c5db3664a7f962076e3",
            "c0d74427539940b3aa19f0c1c102d10f",
            "05c023d73bf142a9ade76b7625dfc7b6",
            "51b1c121ac5a47d6bd2cf980f9c321cc",
            "22a9e1831cd44c5f826107b3924a65c6",
            "e2b49d3f37f348e7aecf17566947a6fa",
            "7b4f118031b748c881298cfcac1c3ab4",
            "76d2ea6d03094052a5c5a429842eaad7",
            "fd2376945c7b49e2a27cba1c5c0c4bd3",
            "b5ac2696e2444e1eb2653107e1418239",
            "c9a8db8532734c018d62b18c12da50ee",
            "a958ef15f6e64be3a33a5f111b3bafa5",
            "782f239fa71c4153bf0d48a427f43b86",
            "6d6deec61cdd4ec09971dcf08d884142",
            "3f30bd1b88854e7198f03c12070b0719",
            "f573f3e107ca4b6396398cb9e0aaab92",
            "57f6c5eac166457aac695062951aae72",
            "5de38a0c63c24f34ae9ce29ef0e3cfb4",
            "76e8be211e2b41a7a4b6067feb4d01ab",
            "8ff2865fcc6144f682e6b78fdc7bbc17",
            "2cf53d6ba72f4def8bad26dfb03be6a5",
            "97c4307ee54e4d488ec24e0b00d4639c",
            "063917b5dfec40428d9c7e86f9e95bc5",
            "5bfe9c00103c40cda5c1804719e33fad",
            "cf36fdef6f9e42c883ce1bc75f20c513",
            "0aa8efacaefb4e91a9070b1abefd8ff7",
            "215550fef45a4254b10f8879c8ee01e0",
            "e0732a54540f4019a00a071e34910f1d",
            "1ca7cd8efec842d7977117412b0bbcc6",
            "5451c572c902418aab7ce7b925beecfd",
            "518b70810a2d4e369e16ff0f48cea4b7",
            "b6eb5f059ded4aab99d958ad94458bd4",
            "3c9caadf12b7467aa922ecaf31255aa0",
            "ad2da204f7d34057b14871bc41f95daa",
            "590ddd4c953740eb8db7dc5536de5a15",
            "aa4f58d2634849dfac9b2a9b2b7785ee",
            "91973c47ff66417eafa1dd9b28f7435c",
            "e7a7913e4ce745298b453c661b281e80"
          ]
        },
        "outputId": "86fec57d-b3e8-4662-90db-b2d302a41782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "894383eb3645419aba3dc2f0c45b3365"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e2b49d3f37f348e7aecf17566947a6fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57f6c5eac166457aac695062951aae72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e0732a54540f4019a00a071e34910f1d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=dtype,\n",
        "    load_in_4bit=True,                  # 4-bit quantization (bitsandbytes)\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "def chat(\n",
        "    messages,\n",
        "    max_new_tokens=100,\n",
        "    temperature=0.2,\n",
        "    top_p=0.95,\n",
        "    presence_penalty=0.0,\n",
        "    frequency_penalty=0.0,\n",
        "):\n",
        "    \"\"\"\n",
        "    messages: list[dict] with roles: \"system\" | \"user\" | \"assistant\"\n",
        "    returns: generated assistant string\n",
        "    \"\"\"\n",
        "    # Qwen uses a chat template in its tokenizer\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            do_sample=(temperature > 0),\n",
        "            repetition_penalty=1.05,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Slice off the prompt portion\n",
        "    gen_ids = output_ids[0, input_ids.shape[-1]:]\n",
        "    text = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "XIWfH2Yru4t_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "c76785bfe82c43ff9e9f01ec0e761008",
            "9f7ba93804ef4371864942ff6f5607b8",
            "f79c4f2ec54241b88e9c03afd46b339f",
            "d3b081615e2149b3b7b47cdbb278405c",
            "ba9c5c6f7ac743f49c65743e4aa3b065",
            "33e8fd2249fb41d4b8a6f4a0b3dad6c7",
            "f3b1d4e2808a4e14855a674079d36657",
            "0d7de919d2ef4c33bcae6605b3203309",
            "e0bd7ecf4d1940548eb0168bc63fd3af",
            "61e054aa89384109bebbc38b5885ea0b",
            "7724401f93464c169519b66449f96c85",
            "af24f68c0ffb4bb6823b7b640f328098",
            "bf1fa948098846ccb875c45687456293",
            "49cc83bf08b846ce95ecd3f7af2502d2",
            "37211b5360b240e1892f3a859989389d",
            "06bcad27fbba486a8a29575c3270fba3",
            "98ef417e611146a1aafd3ecda4922981",
            "7510b8ba44694644a3ccae300d0696c5",
            "fe39b683a7e946428fb07c022c77c57a",
            "e55939608a7f42ce91ccdd3d8ea702f6",
            "072af052024e42fda9c165f4d72e500d",
            "6b416e7cfeb34efe9ca00eb6765d6e21",
            "0a60a97c31f8406dbc3856da38bdd44e",
            "01058c59f8024b5895d33c45864fb5e9",
            "2300e319175e432b8317bd5a51e1f4c7",
            "b0737a39b65848e19f376719ce26ad4d",
            "a222c1cc72104f8586b4a8d5adb07bdd",
            "c66e2d5189e7409ab87be946deb7780d",
            "a11753a36489434689f4a582dd1ec0bc",
            "a6590c04095d4678adc824461be9e899",
            "999bd760f167407680c7187cf624323c",
            "45bcc2ff48a74fa4b871d87e588115f7",
            "8e9de7671d024eceade0b472d7f3f3eb",
            "d4d6373633374c8c82463219b90294d3",
            "e233b62f986749beab3e9a54af86d228",
            "f3d3a73a306b4be9bd21d5436fdb33b8",
            "c80d91ba926b4252bd077164ee38ad1a",
            "ca1d3032bac24959b2833054e8ba363e",
            "1b7b1c8109854ce1aff6ea53c2b4a70c",
            "123c9e635a0946329f3c7f2053920c1e",
            "c2021ff88db94d1e860b6cca19b8932b",
            "171a0de2cbe44a6ebebf2a6f0de3085e",
            "3aa0d61fef9e4e9bbb5cebd7931a491e",
            "b5c05631b3b34dba92c417c21206dc5f",
            "804f43c11f9f4e56bfad8a884fb4c008",
            "cdd181b778c64e349f1da0b5b9dbcc21",
            "b9956102f4dc4e5ab22356811f756533",
            "d99c29c62a23498e900c4b56c715000c",
            "a1e40d9e4d434ae8b799b097a2c71bbb",
            "fc9de9d1e4604730a260aed1a29dcc57",
            "40d92617da1b4c38a04a8fdcd08bb2db",
            "9698a65353784e82a54df6830d591e35",
            "ce9799dc0a694850a4016c953b5a4762",
            "a0fc05a19ccf49cca5f6ebc7cce89326",
            "45154a4107c44116b8d7c6836c68bd82",
            "130d4b2e10bb4fb8bd687827a0f0139f",
            "740578835a994a44800acab53221c4d4",
            "a3f5514ee65e49198c71285984d1af6c",
            "8dae47280f3046f6bb55e43aa7d5410c",
            "db1ab360131b49b9a24eab69fb413e34",
            "1383ab4689944f63ab370276cc4d21bb",
            "cfabaa92f5524e649df5d1a5a04a430e",
            "77f54509c16a4a7d8c5e0cc439d13931",
            "16405d7056004fffb9c9ce43a1639abd",
            "c6e2fd2d78d0449db18cdd022d2b2dd5",
            "0cf425c276a04ab8a2bdd00b67ce4c3f",
            "c02bf9f9a3e847e4a635be002df9da0b",
            "b8d5f5ac72244d8aa95725e7ac9e290a",
            "6a9cae34984b492bbad8f2b9ff1ac798",
            "a40086f8f77440fc8ebfee194abc8db6",
            "c801ab9b484f40f8b1b0eecc0950ba52",
            "a148624bc64d4ff9a001d098e8240ca6",
            "00b19d8048b64e6cbe053fd11c04d094",
            "d27901166a0146f0bb11def9babd7ab5",
            "dd1c31009f2c45dc9551942751e0c5a4",
            "054357b309ef45dd845ec876c84f6661",
            "732e81ac3fcd417cb4cc55500171e9f1",
            "35c7eabeb7604bd6a071c4a50c6b1fbb",
            "b1df9f474bf14468b92a6a0530f5de72",
            "9858f74d19f84500be113888f9425053",
            "960bccfb1b744a26a517bac1e1138dc3",
            "b6b1affcdec146a4a54c7d92d0faae8d",
            "bb898a216316444dab7002e0f7899498",
            "5d3ce933c9af489a9c3a817debf34f5d",
            "034cc20cddff4604a36c85ccd0ab21d0",
            "5c788e9c817344cf9fa88bf46f088b13",
            "183ed278e8cc4e71bff710a2de01d814",
            "ca8c1377ca764817811878e887abe9b5",
            "8c4e3e2871fd44b58293aa328401575f",
            "051adb1e26214f9b8fa7dd2d9fb494a5",
            "966df7fc78f24fc1a817cc81d2f8f991",
            "8b0db5a5747e4051bee2f93738621ce2",
            "5b05fd9c6a26463285d4e36d5393b2e7",
            "a2900a67677f41e09ee33c68f5a17a8f",
            "835287eb739841418956c5e9c8624a0e",
            "303e8fbbbdf14c99b40e6e680b425235",
            "9c7e3e6952884ca9b0292345d21d4a8e",
            "826062faa53a4590a1a80864d972a23c",
            "5da1059db83544afb91e246a0f78da16"
          ]
        },
        "outputId": "1f10d9fe-a951-459e-c9af-47b266ef042f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c76785bfe82c43ff9e9f01ec0e761008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af24f68c0ffb4bb6823b7b640f328098"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a60a97c31f8406dbc3856da38bdd44e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d4d6373633374c8c82463219b90294d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "804f43c11f9f4e56bfad8a884fb4c008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "130d4b2e10bb4fb8bd687827a0f0139f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c02bf9f9a3e847e4a635be002df9da0b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35c7eabeb7604bd6a071c4a50c6b1fbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c4e3e2871fd44b58293aa328401575f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a careful reasoning assistant. Think step by step, but keep explanations concise unless detail is necessary.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Solve: A rectangle has perimeter 50 and one side is 12. What is the area?\"}\n",
        "]\n",
        "print(chat(messages, max_new_tokens=256, temperature=0.2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9kSPvFwyGSa",
        "outputId": "7768df69-6bed-474f-8b8d-d7b9f697c780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's solve this step-by-step:\n",
            "\n",
            "1. First, recall the formula for the perimeter of a rectangle:\n",
            "   P = 2(l + w)\n",
            "   where P is the perimeter, l is the length, and w is the width.\n",
            "\n",
            "2. We're given that the perimeter (P) is 50 and one side (let's assume it's the length l) is 12.\n",
            "   So, we can write: 50 = 2(12 + w)\n",
            "\n",
            "3. Simplify the equation:\n",
            "   50 = 24 + 2w\n",
            "\n",
            "4. Subtract 24 from both sides:\n",
            "   26 = 2w\n",
            "\n",
            "5. Divide by 2:\n",
            "   w = 13\n",
            "\n",
            "6. Now we have the dimensions of the rectangle: length = 12, width = 13\n",
            "\n",
            "7. To find the area, use the formula:\n",
            "   Area = l × w\n",
            "   Area = 12 × 13\n",
            "\n",
            "8. Calculate the area:\n",
            "   Area = 156\n",
            "\n",
            "Therefore, the area of the rectangle is 156 square units.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TU1ede4I8t8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steer reasoning\n"
      ],
      "metadata": {
        "id": "7Ns669z-9_UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip -q install \"transformers>=4.44.0\" \"accelerate>=0.34.0\" \"bitsandbytes>=0.44.1\" \"torch>=2.3.0\" \"einops\" \"sentencepiece\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tv_5zMJCf3S",
        "outputId": "6e7053af-fab5-4c82-ab15-cff4ede88c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, math, re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "dtype = torch.bfloat16 if (device == \"cuda\" and torch.cuda.get_device_capability(0)[0] >= 8) else torch.float16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True, trust_remote_code=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=dtype,\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=dtype,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bae8e83e7595437b92e38d5b05e2ce42",
            "3f15cc78bc724535af7b81446ada1a3a",
            "bb4ee6f195384ccc807fc439b5744a3d",
            "aa4a5a63eef4478c9c6175f84a5d1289",
            "ddd1d1517eb548c2a5d634ad4cd7e1c9",
            "b475ba946aea46d1bbad011cde06f8be",
            "bdf6c8a205d84fc088376b4f055cf56b",
            "0548f0f0577c4e81ae0479a83f4b78ea",
            "2fd3856facd549b19e715a8b4556d185",
            "c1c16fb71a5d43f08deb5597fc2448e2",
            "481c41728df44897a5ddc58ddf321b3d",
            "b7edd4a683434f4e9f34b358bd95b567",
            "6bd1fcfe2b214c6f8d78953a3691207a",
            "871ef08d68fe44f4a07b39686793b735",
            "2d0cd30f2b0f43ad94a73492830be9c8",
            "4c7f56287ac94939ad85221a8c41cb3c",
            "a4ca8c977e5e46fabba35873d3a336cb",
            "450771ac576045b095f85493288c820a",
            "ebfc4ea380c44bcc83ba0e2b1a5f9473",
            "06269b66effe4845885246dbb16c40b1",
            "f94bab1d69e04fe59b9a6f30d3bfb301",
            "292ed9e2a04444db9c97e6e182926aec",
            "ffff54ac5796493295cdc4466b9440cc",
            "2e9f02b4e5e04aa980478b985016e491",
            "b0ad2dae7bf743289a0bddc53e449fee",
            "26b7ea71113b42b5866c5c3ed73271db",
            "6be80fe9a20345d3b5492ea036884987",
            "4e313030b84441cc8104f7b887952374",
            "3e5a8dd1708047aca07ddff7106abb04",
            "6d616f1a68454347bc8e06ece6e9f399",
            "7f0f9dfec01540feae7a955d889cd098",
            "a2c73038e72a4f8f83abc8c417d4413e",
            "36e61ea17cb049e787f8ef315b4082ed",
            "f4cb273ba9d94da0a65a00c86f2e46ff",
            "0ae73400be774667a703b19b6ce03e70",
            "889e0ab030aa480192c23e2972801216",
            "6e72f9e4f7f54be5b5bc7f6714c38eb4",
            "07302368d5ea4847b376f8d7f63c161f",
            "da0ef5733244444e89cfaf8287c17ef7",
            "295417377f45419cbb48b17743fb8905",
            "00b5d7539646416099bad2e3b2924fab",
            "d91a524740d0489a8baf2178d06d8775",
            "1b7577d90e7c4c2f9ce371801b061420",
            "405dedc9478044759c564602a8547aee",
            "44bcb21bcfdf496eadf60ebe1ee07211",
            "e89a129a283f47ac9c93d3584b55733b",
            "9ee9f11cf6c74f99b877efd74fbe35fc",
            "7481531d3f2148159ccd6d429990200a",
            "7abcd5bb64634b108d287b4cbd1071e9",
            "281f1ade934a4ba2a0df57544cc88175",
            "e6dcbd3410f74a958deb7d60b03eae7e",
            "4dd93b372f174c76b90a8d51e5306e97",
            "453e756dbe194d2695c8d19da19e3e7c",
            "540435f8a96748a69c63d7160f5b82fa",
            "8c05f36b76374691993893e69f7ef923",
            "de26b0d92a31462ca15fb3f2301e34f6",
            "c136c8780d5c4242b2971d5390ed141f",
            "4b3aa91baac44e7892efc655a6a0820e",
            "d2710609ed584eae8484c015d2b63a4c",
            "c91606054ea843fd86c870330bf6a688",
            "ce0fc5d879e64f2299686f9ceab4372d",
            "69ce0e0ac4264c93912541b8b7744edd",
            "a4523e796c9b4d89a5cea9d94dd5a1e0",
            "e43373f7bb064e39bc56fc00f26da058",
            "1b05305a17bb4edfaa0467952418b89b",
            "fc333d615ab4459da0b76ad60f5a18c0",
            "f3ed35f7c4694b779509940f20679ee1",
            "ae97aae0739047ca8559e63a89bc881f",
            "f25fa55d869d46b49a7fb7e42c0ef47f",
            "86347f85302347bd85c881ff6254a75d",
            "3dfd5c21997e43ffad25fe81f6d626d0",
            "b91ac2f645ac4137b9c4854fd94a13a4",
            "23d43eddf04a4933a5b4b87941ed49c4",
            "838e3d74d1e049cca2fbfbc59ebf053f",
            "ab89334f56524331836786af4f68115c",
            "39b100155bc546c4bc9eafbb6673d9a4",
            "e235618581bf45839ffedfddb6390bec",
            "8e73e94d672b4718b09aa368d6dc8815",
            "7cd9b9fddb914b02a8f7ffab03d4f523",
            "276c645a6b004483b16201f1968687b3",
            "ee86f3ed25544e098033d85955224a84",
            "bf33ff13ddf54eb1917290d1605043f2",
            "f1e3412d1dbf42c0812e9f4f715e1d42",
            "2ec80125b77445f08c7995e24020cf54",
            "799fdda9e09342f097ca32964620fbda",
            "62b708e236f047c7a51926d1f4cdd5e3",
            "d33b4a27a05d4b8b9645d1c1e664f998",
            "35b9241e2d6646d5b1a85eb6d8f4fc4c",
            "d2650eca53a442fab3cebcd4a29b8550",
            "80519ec114d54ad8a62ff9ad0d59920d",
            "01c320f8b1c74d77bf75a980e38bb874",
            "2268d62c71504a5ca8de0973417f5809",
            "b8a9c833709e4774ae8122bfeb47fac4",
            "8a88f9fc12c94a3e8a97fb1b8b3f7116",
            "43ea099f090f4270a94e828a98d1f7e8",
            "2ba2d87208244e0dbd76eb24b3075585",
            "60d6d16a6e144037a2203f74a2aa7a90",
            "8cc241355f30409db99d32c6c8e45062",
            "4fc4feb3f19e471a959f0beaed129e76",
            "5f767a840c1a42bc817aa102739925e9",
            "43790921794147e4b89fe702662b86f3",
            "8422791f83dd44089132fad958c2e77e",
            "601c726aecab4386bb4cb1ce2ca6bcf5",
            "b4b55e66dbc64d108527dcae7f4affc8",
            "4315e979eb354ee9a3767b1c9e892ed8",
            "2af1493e26f9468a8732ce8038241294",
            "af791a2c476842f49a385960ebd74548",
            "f50f8ae6c883447d93db93b4dc91f9b0",
            "800f6ce62f794548bbf551b689b9040b",
            "964f2d2cb69746b181f632029042d575",
            "d63ce88044ca4c73ace9f4cb85f0fbd3",
            "cc4f4be81dc445a2ac5a55cda1ec4f50",
            "2b762870a98641dc87dcd5756ebdf118",
            "827964fed0f0471280f2e4ae5f9b11ae",
            "6faa78f2e0f542ffb4ffffa23611556f",
            "936269a77a9d4340b60ed599528ce4e6",
            "f28f7c7b5f2c463d89a5b5bbb484f821",
            "c566aaa5bb63466ca90ae6e02d2c1103",
            "1277dbfd31fa46bf918e0a1d07f4b44d",
            "2d42472b72064f48a59b4ebce847dbca",
            "fcd5505ef1124d45a820a01b9e82d373",
            "0a9d8472f67045d3af59c8e17490e082",
            "668cecb8d9284b038b02137be87d0110",
            "8fe842e21aab44eeb6d841b0eafb083f",
            "3a76aca3e63349379c5b6d7449cf408e",
            "232a8e24a8184d2d86c68a0d9bf598d1",
            "aa55b85f267c4cc99621d09032b8da87",
            "c8e8962559f648f1943f0a1f0a7e7417",
            "de0ce0b626ef4bfdb4891aafd8f6249c",
            "5d01a9d3e9bd4ea59910b98a7e61c674",
            "aa4d85d2b12740e8b3e7c1469978b680",
            "face0facf2614156b78b972289070ac8",
            "1e77ad0a6fbb4f4bbbc9ddc5e5e40b19",
            "b99c7d643c85448888bc4dae11c070ff",
            "a46ddb859e2547209f89a88766ac8432",
            "5006102610da46fa8b0109e3abe807a9",
            "6b1031d081ab402c85234f2acaffca82",
            "88503720087a4743ac7aabf1f9d31fdc",
            "79aca8c0dbed413db5ab041f52b6bec6",
            "66bb61efdea3474c899dcb91085202aa",
            "06fc6146827e449fa5008907dd0189e4",
            "50c3aff3808647a18a4463b7623f008d",
            "145c28ac1c034148afa15226c0904626"
          ]
        },
        "id": "6Lyf85A8EvoX",
        "outputId": "a304c869-7432-45b0-be84-ef3553ae7b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bae8e83e7595437b92e38d5b05e2ce42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7edd4a683434f4e9f34b358bd95b567"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffff54ac5796493295cdc4466b9440cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f4cb273ba9d94da0a65a00c86f2e46ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44bcb21bcfdf496eadf60ebe1ee07211"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de26b0d92a31462ca15fb3f2301e34f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3ed35f7c4694b779509940f20679ee1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e73e94d672b4718b09aa368d6dc8815"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2650eca53a442fab3cebcd4a29b8550"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f767a840c1a42bc817aa102739925e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d63ce88044ca4c73ace9f4cb85f0fbd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a9d8472f67045d3af59c8e17490e082"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e77ad0a6fbb4f4bbbc9ddc5e5e40b19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_inputs(messages):\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def generate_from_messages(messages, max_new_tokens=256, temperature=0.2, top_p=0.95):\n",
        "    input_ids = build_inputs(messages)\n",
        "    out_ids = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        do_sample=(temperature>0),\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.05\n",
        "    )\n",
        "    gen_ids = out_ids[0, input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "3qyMYQOOEwmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tasks = [\n",
        "    \"Compute 24 * 36.\",\n",
        "    \"If 3x + 5 = 20, solve for x.\",\n",
        "    \"A train leaves at 3pm going 60 mph, another at 4pm going 80 mph. When do they meet?\"\n",
        "]\n",
        "\n",
        "def build_cot_messages(question):\n",
        "    return [\n",
        "        {\"role\":\"system\",\"content\":\"You are a careful reasoning assistant. Always think step by step. Wrap internal reasoning in <thought>...</thought>.\"},\n",
        "        {\"role\":\"user\",\"content\": f\"{question} Show your reasoning inside <thought>...</thought> before the final answer.\"}\n",
        "    ]\n",
        "\n",
        "def build_nocot_messages(question):\n",
        "    return [\n",
        "        {\"role\":\"system\",\"content\":\"You are concise. Give only the final answer, no reasoning.\"},\n",
        "        {\"role\":\"user\",\"content\": f\"{question} Only output the final answer.\"}\n",
        "    ]\n"
      ],
      "metadata": {
        "id": "8RrClRzTEw0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def get_cot_outputs(tasks):\n",
        "    outs = []\n",
        "    for q in tasks:\n",
        "        msgs = build_cot_messages(q)\n",
        "        text = generate_from_messages(msgs, max_new_tokens=256, temperature=0.2)\n",
        "        outs.append((q, msgs, text))\n",
        "    return outs\n",
        "\n",
        "cot_ref = get_cot_outputs(tasks)\n",
        "for q, msgs, text in cot_ref:\n",
        "    print(\"Q:\", q)\n",
        "    print(\"---- CoT output ----\")\n",
        "    print(text[:400], \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1yRox92Ew3r",
        "outputId": "db1c8a74-a0ac-44d3-c2d8-d6d23a1f3047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: Compute 24 * 36.\n",
            "---- CoT output ----\n",
            "<thought>To compute 24 * 36, we can use the distributive property of multiplication over addition. We can break down 24 and 36 into simpler parts:\n",
            "\n",
            "24 can be thought of as 20 + 4\n",
            "36 can be thought of as 30 + 6\n",
            "\n",
            "Using these, we can expand the multiplication as follows:\n",
            "\n",
            "24 * 36 = (20 + 4) * (30 + 6)\n",
            "= 20*30 + 20*6 + 4*30 + 4*6\n",
            "\n",
            "Now we calculate each part:\n",
            "20*30 = 600\n",
            "20*6 = 120\n",
            "4*30 = 120\n",
            "4*6 = 24\n",
            " \n",
            "\n",
            "Q: If 3x + 5 = 20, solve for x.\n",
            "---- CoT output ----\n",
            "<thought>First, I need to isolate x on one side of the equation. To do this, I'll start by subtracting 5 from both sides of the equation to get rid of the constant on the left side.\n",
            "\n",
            "Starting with:\n",
            "3x + 5 = 20\n",
            "\n",
            "Subtract 5 from both sides:\n",
            "3x + 5 - 5 = 20 - 5\n",
            "3x = 15\n",
            "\n",
            "Now that I have isolated the term with x, I can divide both sides by 3 to solve for x:\n",
            "\n",
            "3x / 3 = 15 / 3\n",
            "x = 5</thought>\n",
            "\n",
            "The solutio \n",
            "\n",
            "Q: A train leaves at 3pm going 60 mph, another at 4pm going 80 mph. When do they meet?\n",
            "---- CoT output ----\n",
            "Let's break this down step by step.\n",
            "\n",
            "1. **Initial Conditions:**\n",
            "   - Train A leaves at 3pm and travels at 60 mph.\n",
            "   - Train B leaves at 4pm and travels at 80 mph.\n",
            "\n",
            "2. **Relative Positioning:**\n",
            "   - By 4pm, Train A has already traveled for 1 hour at 60 mph, so it is 60 miles ahead of Train B.\n",
            "\n",
            "3. **Relative Speed:**\n",
            "   - The relative speed of Train B with respect to Train A is \\(80 \\text{ mph} - 6 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_decoder_layers(m):\n",
        "    if hasattr(m, \"model\") and hasattr(m.model, \"layers\"):\n",
        "        return m.model.layers\n",
        "    if hasattr(m, \"transformer\") and hasattr(m.transformer, \"h\"):\n",
        "        return m.transformer.h\n",
        "    raise ValueError(\"Could not find decoder layers list.\")\n",
        "\n",
        "layers = get_decoder_layers(model)\n",
        "num_layers = len(layers)\n",
        "hidden_size = model.config.hidden_size\n",
        "\n",
        "STEER_LAYER_IDX = num_layers // 2\n",
        "print(\"Using layer\", STEER_LAYER_IDX, \"of\", num_layers)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c-4yILmEw6p",
        "outputId": "e3e19de6-b300-4eb3-8d01-e70fb585bf50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using layer 14 of 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Capture:\n",
        "    def __init__(self, layer):\n",
        "        self.layer = layer\n",
        "        self.hiddens = []\n",
        "    def _hook(self, module, inp, out):\n",
        "        self.hiddens.append(out.detach())\n",
        "        return out\n",
        "    def __enter__(self):\n",
        "        self.h = self.layer.register_forward_hook(self._hook)\n",
        "        return self\n",
        "    def __exit__(self, *args):\n",
        "        self.h.remove()\n",
        "\n",
        "def build_prompt_text(messages):\n",
        "    return tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "\n",
        "def encode_with_hook(full_text, layer):\n",
        "    toks = tokenizer(full_text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.inference_mode(), Capture(layer) as cap:\n",
        "        _ = model(**toks)\n",
        "    return toks, cap.hiddens[-1][0]  # (seq, hidden)\n"
      ],
      "metadata": {
        "id": "psGa1YFSEw9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_span_indices(text, start_tag=\"<thought>\", end_tag=\"</thought>\"):\n",
        "    m1, m2 = re.search(re.escape(start_tag), text), re.search(re.escape(end_tag), text)\n",
        "    if not m1 or not m2: return None\n",
        "    return m1.end(), m2.start()\n",
        "\n",
        "def mean_over_span(hidden, start, end):\n",
        "    start, end = max(start,0), min(end, hidden.shape[0])\n",
        "    return hidden[start:end].mean(0) if end>start else None\n",
        "\n",
        "cot_means, nocot_means = [], []\n",
        "\n",
        "for q, msgs_cot, cot_text in cot_ref:\n",
        "    span = find_span_indices(cot_text)\n",
        "    if not span:\n",
        "        print(\"[skip]\", q)\n",
        "        continue\n",
        "\n",
        "    full_text = build_prompt_text(msgs_cot) + cot_text\n",
        "    toks, acts = encode_with_hook(full_text, layers[STEER_LAYER_IDX])\n",
        "    total_toks = toks[\"input_ids\"].shape[1]\n",
        "    # crude but works: take last ~60 tokens of the answer as reasoning span proxy\n",
        "    v_cot = acts[-60:].mean(0)\n",
        "    cot_means.append(v_cot)\n",
        "\n",
        "    msgs_nocot = build_nocot_messages(q)\n",
        "    text_nc = build_prompt_text(msgs_nocot) + \" The answer is\"\n",
        "    toks_nc, acts_nc = encode_with_hook(text_nc, layers[STEER_LAYER_IDX])\n",
        "    v_nc = acts_nc[-30:].mean(0)\n",
        "    nocot_means.append(v_nc)\n",
        "\n",
        "print(f\"Collected {len(cot_means)} CoT and {len(nocot_means)} NoCoT samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99Huv10wExAn",
        "outputId": "1e1099a8-4ecc-4fd1-aea1-7ad1e53383ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[skip] A train leaves at 3pm going 60 mph, another at 4pm going 80 mph. When do they meet?\n",
            "Collected 2 CoT and 2 NoCoT samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_mean(vectors):\n",
        "    return torch.stack(vectors).mean(0)\n",
        "\n",
        "mu_cot, mu_nc = robust_mean(cot_means), robust_mean(nocot_means)\n",
        "steer_vec = mu_cot - mu_nc\n",
        "steer_vec = steer_vec / (steer_vec.norm() + 1e-9)\n",
        "steer_vec = steer_vec.to(model.device)\n",
        "print(\"Steering vector norm:\", float(steer_vec.norm()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEimEbhOExDX",
        "outputId": "48ccc9ab-2905-46e7-d9e4-80b840dc0f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steering vector norm: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SteeringHook:\n",
        "    def __init__(self, layer, steer_vec, alpha=0.0):\n",
        "        self.layer, self.vec, self.alpha = layer, steer_vec, alpha\n",
        "    def _hook(self, module, inp, out):\n",
        "        if self.alpha != 0.0:\n",
        "            out[:, -1, :] += self.alpha * self.vec\n",
        "        return out\n",
        "    def __enter__(self):\n",
        "        self.h = self.layer.register_forward_hook(self._hook)\n",
        "        return self\n",
        "    def __exit__(self, *a):\n",
        "        self.h.remove()\n",
        "\n",
        "def steered_generate(messages, alpha=0.0, max_new_tokens=200, temperature=0.2):\n",
        "    input_ids = build_inputs(messages)\n",
        "    with SteeringHook(layers[STEER_LAYER_IDX], steer_vec, alpha):\n",
        "        out_ids = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=0.95,\n",
        "            do_sample=(temperature>0),\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    gen_ids = out_ids[0, input_ids.shape[-1]:]\n",
        "    return tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n"
      ],
      "metadata": {
        "id": "MXg74NanExGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rectangle_messages = [\n",
        "    {\"role\":\"system\",\"content\":\"You are a math tutor. Be correct and clear.\"},\n",
        "    {\"role\":\"user\",\"content\":\"A rectangle has perimeter 50 and one side is 12. What is the area?\"}\n",
        "]\n",
        "\n",
        "for alpha in [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]:\n",
        "    print(f\"\\n========== α = {alpha} ==========\")\n",
        "    print(steered_generate(rectangle_messages, alpha=alpha))\n"
      ],
      "metadata": {
        "id": "SwbAFg62ExJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(steered_generate(rectangle_messages, alpha=60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPgz67PpHHlv",
        "outputId": "9f795cc8-861b-48c2-bb82-6056e46b65df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the area, we need to use the information about the sides of the rectangle. The perimeter is given by:\n",
            "\n",
            "\\[ P = 2 \\times (length + width) \\]\n",
            "\n",
            "Since we know the length and width, we can solve for each side:\n",
            "\n",
            "\\[ length = \\frac{P}{2} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\times (length + width)} = \\frac{2 \\ \\times (length + width)} = \\frac{2 \\ \\times (length + width)} = \\frac{2 \\ \\times (length + width)} = \\\n"
          ]
        }
      ]
    }
  ]
}